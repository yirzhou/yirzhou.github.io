[{"categories":null,"contents":" Recursion: where it all began The tale of dynamic programming all started from recursion, a cousin of iteration. To illustrate the how they can achieve the same thing, let\u0026rsquo;s imagine that we have a 0-indexed list with $n$ elements:\nLet\u0026rsquo;s start with something simple: let\u0026rsquo;s get the $k_{th}$ element of the list given the head iterator of the list. Normally, one would start with a while or for loop:\n1 2 3 4 iter = head for i in range(k): iter = iter.next() print(\u0026#34;Kth element\u0026#34;, iter) While we are satisfied with the above solution, one can also do it recursively.\n1 2 3 4 5 6 7 8 def get_kth_elem(iter, num): if num == 0: print(\u0026#34;Kth element\u0026#34;, iter) return get_kth_elem(iter.next(), num-1) get_kth_elem(head, k) The above two procedures accomplish the same thing, but the recursive one is clearly demonstrating it\u0026rsquo;s true definition:\n(Resursion is a method of solving a computational problem where \u0026hellip;) the solution to a problem depends on a smaller instance of the same problem.\nIn the above example, printing the $k_{th}$ element of the whole list is the same as printing the $k-1_{th}$ element of the same list if we start from the second element as the head, an is the same as printing the $k-2_{th}$ element of the same list if we start from the third element as the head, and so on. This thinking process might sound unnecessarily redundant, its beauty will shine once one realizes that it exists in many places in our world.\nFor now, let\u0026rsquo;s say that a procedure is recursive if the procedure depends on a smaller instance of the same procedure.\nTrees: a recursive data structure When a construct, or a data structure exhibits the trait similar to a recursive procedure, in which a part of this construct consists of one or more smaller instances of the same construct, we say that the construct is also recursive. A prime example that one in the field of computing often comes up with is a tree. We usually draw a tree from the top to the bottom instead of the other way around.\nWhy is it recursive? If we view every node and its decendants as a tree, then the root node represents the whole tree. In the above example, its root node has two child nodes $C_{1,1}$ and $C_{1,2}$ with some decendants each, so they are also trees that represent a portion of the whole tree. We call them subtress. Similarly, their children are also subtrees (or sub-subtrees if you are rigorous) of the whole tree, until we reach to a leaf node (i.e., the node without any children) which is the simplest form of a tree.\nTo construct a tree, we can use a recursive procedure. We start from the top, root node. But, to construct the root node which is the entire tree, we need to construct its children. Then, we go to each of its children, from which we need to go further down to construct their children. In other words, the procedure of constructing the tree consists of some procedures of constructing smaller trees. This procedure is thus recursive. A rather non-rigorous piece of code can be used to perform this procedure:\n1 2 3 4 5 6 7 8 9 10 def construct_tree(depth: int): node = Node() # Suppose we want to stop here as we\u0026#39;ve reached our desired depth. if depth == target_depth: return node children = [] for i in range(children_count): children.append(construct_tree(depth+1)) node.children = children return node Interestingly, the visitation procedure to go through every node in a tree is following the same pattern as the construction procedure:\n1 2 3 4 5 6 7 def visit(node): # We are visiting the current node. # For now, let\u0026#39;s just print it. print(node) # Then, visit all children for child in node.children: visit(child) Again, to visit the entire tree, we visit every node, from which we visit every child of it - recursively.\nA careful reader will notice that for every node, we grow its sub-tree only once. This is due to the nature of a \u0026ldquo;tree\u0026rdquo;: for every node, it only has one parent. This means that if we are visiting a node, the only path via which we get led here is through its sole parent. We can extend this logic via induction to prove that using the above recursive procedure visits every node only once.\nOne would also notice that the very first example of a list, is also a recursive data structure: every element has one predecessor (except the first) and one supercessor (except the last). In fact, we can visit our list in this procedure given an iterator:\n1 2 3 4 def visit_list(iter): if iter is None: return visit_list(iter.next()) Now comes the true nature of dynamic programming.\nDAG, the true nature of dynamic programming Now, a curious reader would want to know: what if we modify our tree in a way that each node may have more than one parent. Well, it can\u0026rsquo;t be called as a tree, can it? Such a \u0026ldquo;tree\u0026rdquo; doesn\u0026rsquo;t exist in nature anyways. In fact, in the discipline of Graph Theory, we have another name that generalizes such constructs: Directed Acyclic Graphs (DAGs).\nLet\u0026rsquo;s look at every word in this term and check if a tree exhibits the concept of that term:\nDirected: a tree is inherently directed - we mentioned that we grow a tree downward (of course, one can grow it upward just like a real tree). There\u0026rsquo;s no tree that grows backward to its root in nature. Acyclic: it means that there\u0026rsquo;s no cycle in this graph. A tree by definition cannot have cycles. Graph: a tree is a type of graph/network, a more specific type of graph where 1) we specify the direction to go from one node to another, and 2) that every node has just one (or no) parent, and 3) that the graph has only starting point (the root) from which we can visit all nodes. A DAG is just like a tree, except the fact that a node may have more than one parent, and that the graph may not have a single point from which we can visit all nodes. Now, if one wishes to perform the visitation of a DAG using the previous procedure, where should she/he start if there\u0026rsquo;s no single starting point? Well, we\u0026rsquo;ve got to try every node (possibly as few as one can to save some effort) from which one can visit the entire graph. In the above picture, one would chose $A$ and $B$ as the starting points.\nBut, if one is cognizant of the cost of using the same recursive pattern as that for a tree, one will see that some nodes will be visited multiple times:\nFrom $A$, we visit $C$, from which we visit $F$, from which we visit $G$, from which we visit $I$, from which we visit $J$; then, from $F$ we visit $H$ then, from which we visit $K$, from which we visit $I$, from which we visit $J$; then, from $C$ we visit $E$, from which we visit $F$, from which we visit $G$, \u0026hellip;\nThis is getting absurdly long, but one can clearly see that several nodes are visited repeatedly. This is particularly problematic if the DAG is huge (large number of nodes / edges). What if we come up with an optimization technique such that one will remember the nodes (or subgraphs) that have been visited when we come to them for the second, third, fourth, \u0026hellip; time?\nWell, that is what dynamic programming is all about. It\u0026rsquo;s an optimization technique used to save states that have already been computed to avoid repeated computation. One can come up with the following optimized visitation procedure for a DAG:\n1 2 3 4 5 6 7 8 9 10 11 12 visited = {node: False for node in nodes} def visit(node): # Avoid repeated visitation on the same subgraph. if visited[node] is True: return # print the node for now as the \u0026#34;visit\u0026#34; action. print(node) for neighbor in node.neighbors: visit(neighbor) # Remember that it has been visited. visited[node] = True Repeated computation is a main source of inefficiency, in both the field of computing and our real life. This is the beauty of dynamic programming - to save repeated efforts in computing.\nNow, one may ask: what if the graph is cyclic (i.e., has cycles)? Will the above procedure, or dynamic programming work? The answer is: No, unfortunately. To see why it would fail, let\u0026rsquo;s just add a backward edge from $H$ to $F$ on the DAG, which makes it a non-DAG now:\nThis graph has a cycle that includes $F$ and $H$. When one visits $F$, she/he would visit $H$ as the next step; however, to complete the visitation on the subgraph represented by $H$, one would need to visit $F$. This recursion will never end. It will dig a infinitely deep hole in your brain; for a poor computer that is running this procedure, its stack space would overflow with recursive calls\u0026hellip;\nAt this point, I will create my own \u0026ldquo;definition\u0026rdquo; for dynamic programming as the following:\nDynamic programming is an optimization technique used to remember the visitation of nodes in a DAG such that they won\u0026rsquo;t be repeatedly visited.\nI don\u0026rsquo;t particularly find the terms frequently appearing in the definition of dynamic programming, such as \u0026ldquo;optimal substructures\u0026rdquo; and \u0026ldquo;subproblems\u0026rdquo; intuitive. Instead, a DAG can be visualized and reasoned about more easily. Imagine that you are reading \u0026ldquo;Introduction to Algorithms\u0026rdquo;, and you look at the steps to develop a dynamic programming algorithm [1]:\nCharacterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. A DAG is an optimal structure because there\u0026rsquo;s no cycle, implying that repeated visitation is impossible if one saves the computation previously. Every node in a DAG is a optimal solution because the subgraph starting from it is a DAG. While the steps above are absolutely correct and rigorous, if one can effectively translate a problem scenario into a DAG, the above steps can be forgotten completely when developing a dynamic programming procedure.\nNow, I\u0026rsquo;ve come up my own steps for developing a dynamic programming procedure:\nUnderstand the problem scenario. Translate the problem scenario into a DAG where each node represents some state, and each edge between two nodes represents the difference in their computed results. Visit the DAG recursively with the extension where states are remembered to avoid repeated computation. That, is the tale of dynamic programming, where it started from the beautiful concept of recursion, and ends at the abstract but optimal nature of DAGs.\nReferences [1] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms. Cambridge, Massachusett: The MIT Press, 2022.\n","date":"May 27","permalink":"https://example.com/articles/recursion-dag-dp/","tags":["algorithm"],"title":"A Tale of Dynamic Programming: Recursion, Tree, and DAG"},{"categories":null,"contents":"After today\u0026rsquo;s FSE exam, it officially wraps up my experience in the course Foundations of Software Engineering. As the most important course, or the \u0026ldquo;bread-and-butter\u0026rdquo; of the Software Engineering program, I thought I\u0026rsquo;d summarize what I learned from taking the course, both in terms of the technical and the non-technical aspects.\nBackground in FSE The course, titled as \u0026ldquo;Foundations of Software Engineering\u0026rdquo;,supposedly teaches the students the fundamentals of the subject. In my opinion, it mostly fulfills this objective because it takes a very hands-on approach. According to the course information website, students will learn to \u0026ldquo;iteratively define requirements, and architect, design, implement, integrate, test, and deploy a solution\u0026rdquo;.\nTo learn those things, we worked as a team to build an emergency situation networking application, which is basically an application that uses standard web technologies. The most important things here are 1) working as a team, and 2) iterately building the application in a pseudo-real-world setting where requirements are defined and we should deliver accordingly.\nWhat makes the course unconventional to me is the following:\nMore focus on soft skills Requirements are a somewhat ambiguous (will come back to it later), and Strong focus on team work and communication. In fact, the effort that went into the development of the course was well documented in this paper by the professors.\nIn terms of the \u0026ldquo;foundations\u0026rdquo; of software engineering that we learned, they span across engineering methods (e.g., iterative, LEAN, agile, SCRUM, kanban, etc.), design patterns, architectural styles, technical debt, and requirement engineering, and so on.\nNon-technical Lessons These are the lessons I learned from working with people from different backgrounds and working styles.\n1. Work distribution Work distribution across team members can be a difficult. Most of the time, we should optimize and maximize everyone\u0026rsquo;s strength, and allocate the appropriate tasks. However, it has two downsides: 1) It can lead to a person being hyper-focused on just a single aspect of the project all the time and not learning about others\u0026rsquo; work, and 2) inbalance of work distribution depending on the nature of the task (some tasks might be heavy on the frontend, for example).\nTo solve the above issues, learning and adaptation is important. Thus, one must actively take initiatives if he/she wants to get something useful out of the experience. However, to avoid overloading, concerns should be communicated clearly.\n2. Follow the Instructions Surprisingly, following instructions is supposed to be straightforward: when creating design documents with UML diagrams, for example, all we need to do is to follow the proper format. However, people tend to not follow the instructions carefully (me included sometimes) and it has led to unnecessary wastes of time. This is particularlly important when we deal with project requirements. If the use case is defined in a way by the user, we should build our product based on that requirement; we should NOT build it how we like it.\n3. Make Friends This is possibly the best thing about this course - I genuinely enjoyed the way how they teamed us up. It \u0026ldquo;forces\u0026rdquo; different people who did not knew each other to work together. It makes the graduate community feel tighter and neater. I enjoyed spending time with my folks, and I feel fortunate knowing them.\nTechnical Lessons These lessons concern the technical aspects of the projects, such as tooling and design.\n1. Static Analysis In a big project where the majority of the code is written in JavaScript, you want to \u0026ldquo;help your tools help you\u0026rdquo; by using static analysis tools. We adopted TypeScript from the beginning. The main reason is that the code itself is self-documenting, and its integration with VSCode is superb. By stating types, most exceptions are caught at compile-time. Null exceptions are rare at run-time. It did take a while to learn for some of us, but since it\u0026rsquo;s a superset of JavaScript, a person can learn it incrementally.\n2. Importance of Design None of us are as capable as a software architect, so the design of our project was not ideal. However, we did come to the realization early on: consistent implementation depends on a good design. In our case, we used the Factory Method pattern for our core business logic. This allows anyone who comes onboard to implement the logic in a way that is guaranteed to work and be compatible. On the other hand, when we learn something new in class, we\u0026rsquo;d better carry it out in the real world by practicing it.\nConclusion Overall, FSE was a valuable experience to me as someone who has done multiple internships. It was not a perfect experience, but making new friends alone has made it worth my time!\n","date":"Dec 03","permalink":"https://example.com/articles/lessons-learned-fse/","tags":["cmu","software-engineering"],"title":"Lessons Learned from a Project-based Course on Software Engineering"},{"categories":null,"contents":"While you are reading this post, I have probably been in quarantine for several days. Yes! I\u0026rsquo;m home, but in a hotel for now.\nI have a lot of ideas that I want to work on. I\u0026rsquo;m not disclosing any of them, but the gist is that by realizing those ideas, I get to experience the same workflow as before but in a different country with some internet censorships. I will have to naturally get through them if I want to work in the same way as before.\nThe problem The first issue I encountered, right off the bat, was not being able to use Homebrew normally.\nBy default, Homebrew relies on GitHub for their repo distributions. In China, accessing GitHub is extremely slow. I immediately realized this when I attempted to run brew update and it timed out with \u0026ldquo;443\u0026rdquo; errors.\nThe solution Luckily, Homebrew has a huge Chinese community that has established repo sources in China. All I needed to do was to set the sources to one of the Chinese ones for homebrew, homebrew-core, and homebrew-cask. I\u0026rsquo;m using the ones set up by the University of Science and Technology of China:\n1 2 3 4 5 git -C \u0026#34;$(brew --repo)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/brew.git git -C \u0026#34;$(brew --repo homebrew/core)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git git -C \u0026#34;$(brew --repo homebrew/cask)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git I also need to change the environment variable HOMEBREW_BOTTLE_DOMAIN by adding this line to my .zshrc:\n1 export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/ To sum up, I can put the above code into a shell script change_source.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh git -C \u0026#34;$(brew --repo)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/brew.git git -C \u0026#34;$(brew --repo homebrew/core)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git git -C \u0026#34;$(brew --repo homebrew/cask)\u0026#34; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git if [ $SHELL = \u0026#34;/bin/bash\u0026#34; ]; then # 如果你的是bash echo \u0026#39;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/\u0026#39; \u0026gt;\u0026gt;~/.bash_profile source ~/.bash_profile elif [ $SHELL = \u0026#34;/bin/zsh\u0026#34; ]; then # 如果用的shell 是zsh 的话 echo \u0026#39;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/\u0026#39; \u0026gt;\u0026gt;~/.zshrc source ~/.zshrc fi To restore the sources, I have created a neat shell script restore_sources.sh that sets all sources back to GitHub:\n1 2 3 4 5 git -C \u0026#34;$(brew --repo)\u0026#34; remote set-url origin https://github.com/Homebrew/brew.git git -C \u0026#34;$(brew --repo homebrew/core)\u0026#34; remote set-url origin https://github.com/Homebrew/homebrew-core.git git -C \u0026#34;$(brew --repo homebrew/cask)\u0026#34; remote set-url origin https://github.com/Homebrew/homebrew-cask.git Closing thoughts That\u0026rsquo;s it for Homebrew - I will likely encounter more issues in the coming days as I spend more time working, so stay alerted for more :smiley:!.\n","date":"May 16","permalink":"https://example.com/articles/homebrew-change-source/","tags":["homebrew","macos"],"title":"Quarantine Post: Make Homebrew Work in China"},{"categories":null,"contents":"In this article, I will walkthrough the process of incorporating the \u0026ldquo;new\u0026rdquo; AWS JavaScript SDK in a web application, particularly how to use it directly in browsers.\nI wrote this because I found relevant documentation a bit lacking. Also, the idea can be applied to using TypeScript in browsers in general.\nIntroduction to AWS SDK for JavaScript v3 AWS have recently released Version 3 of the SDK for JavaScript, targeting the following issues of its previous version (V2):\nModularized packages for individual services (ie. separate packages for S3, EC2, etc., kind of like microservices) New middleware stack Let\u0026rsquo;s help our tools help us TypeScript over JavaScript The most interesting feature to me, is that this new version is written in TypeScript. We all know that it\u0026rsquo;s easy to write type-unsafe and buggy JavaScript due to its dynamic typing system. One component of our final-year design project is in vanilla JavaScript, and people easily get perplexed when looking at the code because they need a lot of time to figure out what certain variables are. For example, suppose that we have a job scheduler:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Scheduler { jobQueue; constructor(jobs) { jobs.forEach((job) =\u0026gt; this.jobQueue.push(job)); } addJob(job) { this.jobQueue.push(job); } runJobUntilNone() { while (this.jobQueue.length) { let job = this.jobQueue.pop(); job.run(); } } } Although the code above is simple, what exactly is a \u0026ldquo;job\u0026rdquo;? It\u0026rsquo;s pretty intuitive to deduce that jobQueue is an array of jobs, but what is a job? Is it a string? Probably not since string doesn\u0026rsquo;t have a run() function. What type is it?\nApart from loose typing, nobody will stop you if you call addJob by passing a number, string, or any other type. What\u0026rsquo;s worse, is that you will only catch the error at run time, which could lead to millions of dollars of loss for your company\u0026hellip;\nTypeScript allows us to add type annotations and any type violation will be caught at compile time by the TypeScript compiler:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Job { // some member variables constructor(args...){//...} run() {// some logic} } class Scheduler { jobQueue: Job[]; constructor(jobs: Job[]) { jobs.forEach((job) =\u0026gt; this.jobQueue.push(job)); } addJob(job: Job) { this.jobQueue.push(job); } runJobUntilNone() { while (this.jobQueue.length) { let job = this.jobQueue.pop(); job.run(); } } } The main difference is that we have \u0026ldquo;enforced\u0026rdquo; the type of the elements in jobQueue to be of type Job, which seems insignificant. However, if we pass any other type into the constructor or addJob, TypeScript won\u0026rsquo;t compile due to type errors. Hence, those errors are caught at compile time which will absolutely reduce the chance of run time errors associated with types.\nThere are many other benefits of TypeScript, especially when it comes to development tooling. I use VSCode which comes with TypeScript support out of the box. If we use TypeScript properly and provide decent JSDoc on top of our code, the power of VSCode will be unleashed even more, greatly enhancing our productivity.\nA note about TypeScript Browsers only understand JavaScript and cannot directly run TypeScript, so TypeScript needs to be compiled to JavaScript before using it in browsers.\nThis can be easily done by the TypeScript compiler and many other JavaScript module bundlers, such as webpack.\nUsing AWS SDK V3 to fetch from S3 Install TypeScript We need to install TypeScript as our project development dependency:\n1 npm install --save-dev typescript @types/node @types/node will provide us with type annotations in VSCode for various libraries.\nConfigure TypeScript compiler First, we need to configure our TypeScript compiler for our project in a file named tsconfig.json. More information about this file can be found here. Mine looks like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;ES6\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;ES6\u0026#34;, \u0026#34;sourceMap\u0026#34;: true, \u0026#34;declaration\u0026#34;: true, \u0026#34;declarationDir\u0026#34;: \u0026#34;./dist\u0026#34;, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;typeRoots\u0026#34;: [\u0026#34;node_modules/@types\u0026#34;], \u0026#34;lib\u0026#34;: [\u0026#34;dom\u0026#34;] }, \u0026#34;exclude\u0026#34;: [\u0026#34;node_modules\u0026#34;], \u0026#34;include\u0026#34;: [\u0026#34;src/s3.ts\u0026#34;] } As you see, we will put our code in src/s3.ts. We also want to generate the associated declaration file under the dist directory. Since we will use @aws-sdk node modules, we should tell the compiler to remember to look into node_modules to find them by setting moduleResolution. Also, the most important options are probably target and module: target specifies what version of JavaScript TypeScript will compile to, and module specifies how we use JavaScript modules in our code. es6 means that we can use the newer import syntax.\nCode The best way to learn something new is to practice. Let\u0026rsquo;s write a JavaScript library to be used in browsers that fetchs a file to a S3 bucket. I will go with the AWS Cognito Identity Pool which gives users temporary access to your AWS services, which, in our case, are fetching files to a S3 bucket.\nFirst, let\u0026rsquo;s install the modules we need for fetching from S3:\n1 npm install --save @aws-sdk/client-s3 @aws-sdk/client-cognito-identity @aws-sdk/credential-provider-cognito-identity We then import them in our code:\n1 2 3 import { S3 } from \u0026#34;@aws-sdk/client-s3\u0026#34;; import { CognitoIdentityClient } from \u0026#34;@aws-sdk/client-cognito-identity\u0026#34;; import { fromCognitoIdentityPool } from \u0026#34;@aws-sdk/credential-provider-cognito-identity\u0026#34;; Then, we define a function to fetch a file specified by a \u0026ldquo;key\u0026rdquo; from S3. We assume that we know the identity pool ID, the region, and the bucket name. We only require the object key from the user:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 export function fetchFromBucket(key: string) { const region = \u0026#34;ca-central-1\u0026#34;; // Initialize an S3 service with credentials for our identity pool. const s3 = new S3({ region: region, credentials: fromCognitoIdentityPool({ client: new CognitoIdentityClient({ region: region }), identityPoolId: identityPoolId, }), }); // Fetch and print out the object size s3.getObject( { Bucket: \u0026#34;my-example-bucket\u0026#34;, Key: key, }, (err, data) =\u0026gt; { if (err) { console.log(`Error when fetching from bucket: ${err.stack}`); } else { console.log(`Data fetched from bucket. Size: [${data.ContentLength}]`); } } ); } That\u0026rsquo;s our straightforward logic. Our next step is to make it runnable in a browser.\nBundle with webpack I mentioned that TypeScript needs to be compiled to JavaScript to run in browsers. Apart from that, we also need to bundle the AWS modules in use with our library. We can achieve this using webpack, which is a bundler that can bundle any web application asset. The short version of what it does is that it will put all necessary code into one file, which we can use in a browser by including it in an HTML file with a script tag.\nWebpack doesn\u0026rsquo;t understand TypeScript by default, but it has a rich ecosystem and comes with a TypeScript plugin. We also need to bundle JSON files into our code as the AWS SDK uses them, and there\u0026rsquo;s a JSON plugin for that as well. We need to install them as our development dependencies:\n1 npm install --save-dev webpack webpack-cli ts-loader json-loader We then define a configuration file for webpack, named webpack.config.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 const path = require(\u0026#34;path\u0026#34;); module.exports = { target: \u0026#34;web\u0026#34;, entry: { s3: \u0026#34;./src/s3.ts\u0026#34;, }, mode: \u0026#34;development\u0026#34;, module: { rules: [ { test: /\\.ts?$/, use: \u0026#34;ts-loader\u0026#34;, exclude: /node_modules/, }, { test: /\\.json$/, use: \u0026#34;json-loader\u0026#34;, exclude: /node_modules/, }, ], }, resolve: { extensions: [\u0026#34;.ts\u0026#34;, \u0026#34;.js\u0026#34;], }, output: { path: path.resolve(__dirname, \u0026#34;dist\u0026#34;), filename: \u0026#34;[name].js\u0026#34;, library: \u0026#34;[name]\u0026#34;, }, }; The configuration file is easy to understand, and the more important ones are:\ntarget is how our code will be used. We target at the browsers, hence \u0026ldquo;web\u0026rdquo; is the value. It\u0026rsquo;s also the default value. entry specifies the entry point of our library code, which is our code to fetch from S3. Webpack will start with this file and construct a dependency graph. It will get all other modules it uses, and also the modules used by those modules, and so on. It then \u0026ldquo;bundles\u0026rdquo; them into a single file that contains all the code we need to fetch from S3. mode affects the formatting of our bundled file. development will keep our code in a format that is easy to develop. In contrast, production will minify our code completely by removing all comments, whitespaces, newlines, etc. module specifies the plugins. We are compiling TypeScript to JavaScript and also including JSON files, hence we have two rules for them respectively. output apparently specifies the bundled file. Because it\u0026rsquo;s a library, we need to specify the value for output.library. [name] maps to the keys in entry, so our bundled file will be ./dist/s3.js and the function fetchFromBucket is under s3. To use it, we simply call s3.fetchFromBucket(). Finally, we can add a simple build script in package.json:\n1 2 3 4 \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;webpack\u0026#34; }, To build/bundle our code, simply run\n1 2 npm run build # we can also run \u0026#34;webpack\u0026#34; directly. This generates ./dist/s3.js which we can directly import into browsers!\n1 2 \u0026lt;script src=\u0026#34;s3.js\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;index.js\u0026#34;\u0026gt; and in index.js, we can call our function to fetch an object from S3:\n1 s3.fetchFromBucket(\u0026#34;example_object_key\u0026#34;); One more thing This seems a rather long process. It takes time initially but the rest of the team will easily benefit from this workflow. Also, I learned about what module bundlers could do and how to create JavaScript libraries for various platforms.\nAlthough I\u0026rsquo;m not into front-end web development, I\u0026rsquo;ve been amazed by its rich tooling and ecosystem :smiley:.\n","date":"Jan 10","permalink":"https://example.com/articles/aws-js-sdk-v3-in-browser/","tags":["aws","javascript","typescript","webpack"],"title":"Using AWS JavaScript SDK v3 in Browsers"},{"categories":null,"contents":" Background Welcome to 2021. This is my first article for this year and I want to start with a useful tip for C++ unit testing. Unlike other languages, unit testing in C++ has never been as straightforward. However, googletest (also known as \u0026ldquo;gtest\u0026rdquo;) has been one of the more well-received testing framework in various organizations, so that\u0026rsquo;s what I will focus on.\nWhat makes this article slightly different is that unlike the traditional way of defining an entry point for your testing program (ie. a main() function), I will walk through how NOT to do that - how to use googletest with CMake without defining any entry point.\nTest case Let\u0026rsquo;s suppose that I want to test some function. The standard way to define a test case using gtest is the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Component under test #include \u0026lt;myproject/maths.h\u0026gt; // gtest include #include \u0026lt;gtest/gtest.h\u0026gt; using namespace ::testing; TEST(my_function_test, basic) { // Assume that I want to test if myproject::abs() // returns the correct absolute value of an integer ASSERT_EQ(myproject::abs(-100), 100); } As you see, I have defined a test case called my_function_test.basic and it tests if abs returns the correct absolute value - nothing exciting.\nHowever, this is just a function - to run our test case, naturally we want to have some entry point just like any program. There\u0026rsquo;s one way to do it: define a main function somewhere like this:\n1 2 3 4 int main(int argc, char **argv) { ::testing::InitGoogleTest(\u0026amp;argc, argv); return RUN_ALL_TESTS(); } While this works, there\u0026rsquo;s a neat way to get rid of this main function completely while still being able to run our tests.\ngtest_main A main function seems out of place being placed in a directory specifically for test cases. Fortunately, Google agrees with this idea and they\u0026rsquo;ve provided the gtest_main library that gives a basic implementation of main(). It means that we don\u0026rsquo;t need an explicit entry point in our program.\nCMake It\u0026rsquo;s simple to use gtest_main with CMake:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 file(GLOB_RECURSE SRCS CONFIGURE_DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp) set(target, my_project.t) add_executable(${target} ${SRCS}) find_package(GTest REQUIRED) include(GoogleTest) target_link_libraries(${target} PRIVATE my_project gmock GTest::GTest GTest::Main ) gtest_discover_tests(${target}) Or it coule be simpler:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 file(GLOB_RECURSE SRCS CONFIGURE_DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp) set(target, my_project.t) add_executable(${target} ${SRCS}) target_link_libraries(${target} PRIVATE my_project gmock gtest gtest_main ) gtest_discover_tests(${target}) After building our project, simply run ctest in the build directory and all test cases will run :smiley:.\n","date":"Jan 05","permalink":"https://example.com/articles/googletest-with-cmake/","tags":["gtest","cpp","cmake"],"title":"Googletest with CMake"},{"categories":null,"contents":" This is one of the most valuable skills that I learned in my current internship. We all love VSCode but this just blew me away.\n1. Motivation As a software engineer working with various languages and their ecosystems, it\u0026rsquo;s easy to mess up your development machine when installing dependencies directly on it. Even when we just want to try out a new language, we still need to have that language runtime installed somewhere on our system.\nIdeally, we want to keep it separate such that it will leave the rest of our system intact.\n2. My story My current work is entirely C++-based: I write code in C++ and I build my project using CMake and Ninja. I develop on a MacBook Pro but my project targets Linux. C++ on MacOS is different from Linux in terms of their compilers and standard libraries: Apple develops their own toolchains using LLVM and uses Clang as its compiler, while most Linux distributions use GCC. There are subtle differences between their standard libraries as well in terms of supporting the newer C++ standards.\nMost people should never worry about those nuances, but they happened to cause an obstacle for me: my project could only build on Linux because some C++17 features were missing in the Clang standard library (libstdc++).\nFor a while, I was working on a remote development machine using SFTP and the command line. It was fairly unproductive: SFTP heavily depends on how fast my network is, and building and tracing bugs down in the command line require very sharp eyes which I don\u0026rsquo;t have :sweat_smile:. Also, I don\u0026rsquo;t have all the features in VSCode properly set up when developing remotely. In short, it was a real struggle for me.\n3. How about containerization? You might have been laughing at my naïveté for a while now: why not working in a Docker container? We can run any operating system and install any dependencies in an isolated environment.\nIn fact, I did try developing inside a container locally, which got rid of SFTP entirely. However, I was still using the clunky CLI and missing the \u0026ldquo;Intellisense\u0026rdquo; and all the other goodies of VSCode.\nIt all changed when I was introduced to Remote-Containers.\nRemote-Containers is a VSCode extension that lets me develop inside a Docker container, using VSCode, as a full-featured development environment. That means that I can enjoy all the features that I need while developing inside a container:\nFull Intellisense and auto-completion with C++ STL CMake extensions Clang-format for code formatting on save You could read more about it if you are interested, and I highly encourage you to get on board. It essentially allows us to open a container in VSCode as a regular file system, and install our favourite extensions and configure settings in a isolated environment.\n4. Setup In the following sections, I will walk through my routine setup for C++ development using VSCode Remote-Containers; however, this skill can be transferred to any other languages or technology stack. I assume that the reader is familiar with container\n4.1 Pre-requisites Docker Engine Visual Studio Code 4.2 .devcontainer.json First, we need to define how VSCode can build and open up a container in a special file: .devcontainer.json. This file can be standalone in your root project directory, or in a separate .devcontainer directory (.devcontainer/devcontainer.json, without the first dot if in the directory).\nWe can set values of plenty of properties that define the locations of docker-compose.yml, Dockerfile, on-init and post-init commands, extensions we wish to install, and many more. For me, I have the following setup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \u0026#34;dockerComposeFile\u0026#34;: [\u0026#34;../docker-compose.yml\u0026#34;, \u0026#34;docker-compose.override.yml\u0026#34;], \u0026#34;initializeCommand\u0026#34;: \u0026#34;mkdir -p debian \u0026amp;\u0026amp; cat */debian/control \u0026gt; debian/control\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;dev-env\u0026#34;, \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/workspace\u0026#34;, \u0026#34;extensions\u0026#34;: [ \u0026#34;ms-vscode.cpptools\u0026#34;, \u0026#34;ms-vscode.cmake-tools\u0026#34;, \u0026#34;twxs.cmake\u0026#34;, \u0026#34;ryanluker.vscode-coverage-gutters\u0026#34;, \u0026#34;pucelle.run-on-save\u0026#34;, \u0026#34;xaver.clang-format\u0026#34; ], \u0026#34;settings\u0026#34;: { \u0026#34;http.proxyStrictSSL\u0026#34;: false, \u0026#34;C_Cpp.default.includePath\u0026#34;: [\u0026#34;/usr/include\u0026#34;, \u0026#34;/workspace/**\u0026#34;], \u0026#34;C_Cpp.default.cStandard\u0026#34;: \u0026#34;c11\u0026#34;, \u0026#34;C_Cpp.default.cppStandard\u0026#34;: \u0026#34;c++17\u0026#34;, \u0026#34;C_Cpp.default.intelliSenseMode\u0026#34;: \u0026#34;gcc-x64\u0026#34;, \u0026#34;C_Cpp.updateChannel\u0026#34;: \u0026#34;Default\u0026#34;, \u0026#34;clang-format.style\u0026#34;: \u0026#34;Google\u0026#34;, \u0026#34;clang-format.fallbackStyle\u0026#34;: \u0026#34;LLVM\u0026#34; } } I put my .devcontainer.json in the .devcontainer directory, and I have two docker-compose files. Some notable areas are:\n\u0026quot;workspaceFolder\u0026quot;: \u0026quot;/workspace\u0026quot; specifies the workspace directory inside the container. I will be running Ubuntu, and it will locate me in /workspace. I will also mount my project to this directory later. extensions specifies a list of VSCode extensions (C++-specific) to install. Note that those extensions will be separate from our editor extensions on our machine and are exclusive to this container. settings specifies container-specific VSCode settings. 4.3 docker-compose.yml Docker Compose enables defining and running multi-container Docker applications. I personally think that its main feature is to spin up containers in an order if there are dependencies between them. For the purpose of this setup, I will only spin up one container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 version: \u0026#34;3.7\u0026#34; services: dev-env: build: context: . dockerfile: dev.Dockerfile volumes: - .:/workspace:z working_dir: /workspace environment: - CTEST_OUTPUT_ON_FAILURE=1 - GTEST_COLOR=1 - CMAKE_GENERATOR=Ninja command: sleep infinity This file has minimal configurations: the most important part is volumes that mounts my project directory to /workspace in the container. I also set some C++-specific environment variables for CMake.\nsleep infinity allows the container to be running instead of exiting immediately which will cause an error.\n4.4 Dockerfile The last missing piece is the Dockerfile (in my case, dev.Dockerfile) which defines the build steps of my container:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 FROM ubuntu:latest LABEL description=\u0026#34;Development environment workspace\u0026#34; ENV TZ=Etc RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone RUN echo \u0026#34;root:docker\u0026#34; | chpasswd RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --force-yes \\ build-essential \\ clang-format \\ devscripts \\ equivs \\ g++ \\ gdb \\ libssl-dev \\ ninja-build \\ openssh-server \\ rsync \u0026amp;\u0026amp; \\ apt-get clean # Copy just the control file. COPY ./debian/control /tmp/debian/control # Install project build dependencies RUN mk-build-deps -i -t \u0026#34;apt-get -o Debug::pkgProblemResolver=yes --no-install-recommends -y\u0026#34; /tmp/debian/control \u0026amp;\u0026amp; \\ apt-get clean # install quantum WORKDIR /opt/ RUN wget https://github.com/bloomberg/quantum/archive/v2.1.tar.gz \u0026amp;\u0026amp; \\ tar -zxvf v2.1.tar.gz \u0026amp;\u0026amp; \\ cd quantum-2.1 \u0026amp;\u0026amp; \\ cmake -Bbuild DQUANTUM_ENABLE_TESTS=ON . \u0026amp;\u0026amp; \\ cd build \u0026amp;\u0026amp; \\ make install EXPOSE 22 CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;] There\u0026rsquo;s not much to say about this file - you can run different runtimes or operating systems of your choice.\n5. How it feels like In short, it\u0026rsquo;s awesome.\nI can open my project in a container:\nMy C++ extensions are working as expected - I get Intellisense, CMake, and Clang-format:\nI can build using CMake (non CLI):\nWhat helps me the most, is navigating through errors - I never need to go into the CLI any more:\nI don\u0026rsquo;t use a lot of tools for my C++ development setup, but I have already realized how amazing it is for my productivity. If your tech stack requires more tools (I\u0026rsquo;m aware that Java and JavaScript have much richer ecosystems), you are going to appreciate it more.\n6. Final thoughts When I started my internship, I failed to realize its significance in my productivity until much later. This is such an elegant approach to solve dependency issues while retaining all the goodies of VSCode, and it demonstrates perfectly the principle of using the right tools to solve the right problems.\nThere are some imperfections about this setup, but most of them are not related to VSCode but to containerization:\nIf you are using a corporate VPN at work, you should consult with your IT department to circumvent it with proxy configurations in order to download VSCode server and extensions. It\u0026rsquo;s not an issue, but definitely something to keep in mind. If the project is huge, you may need to allocate more memory to the container. In my experience, GCC failed during compilation because of the insufficient RAM. I wish I would know it earlier, but I\u0026rsquo;m still glad that I do now!\n","date":"Dec 15","permalink":"https://example.com/articles/vscode-remote-containers-cpp/","tags":["cpp","cmake","container","vscode","technical"],"title":"Perfect Sandbox Environment for C++ Development"},{"categories":null,"contents":"Life is short and wonderful, so I should document it, just like how I treat my day job.\n","date":"Dec 01","permalink":"https://example.com/about/","tags":null,"title":"About Me"}]